{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking을 위한 패키지 vecstack\n",
    "\n",
    "@ http://j.mp/34Etidr & http://j.mp/2NJQ9NO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:red;'>아래 명령어들로 xgboost & vecstack를 설치하고 Jupyter notebook 재부팅(Kernel Restart)을 진행해주세요</span>\n",
    "- 설치 중 에러가 발생하면 cmd(명령프롬프트) 우클릭 & 관리자권한으로 실행 후 명령어를 입력해 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost==1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vecstack==0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1. Usage. Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "import warnings                 \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecstack import stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load demo data \n",
    "\n",
    "iris = load_iris() \n",
    "X, y = iris.data, iris.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train/test split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 1-st level models. \n",
    "# Caution! All models and parameter values are just \n",
    "# demonstrational and shouldn't be considered as recommended. \n",
    "\n",
    "models = [ \n",
    "    ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), \n",
    "    RandomForestClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3), \n",
    "    XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [ExtraTreesClassifier]\n",
      "    fold  0:  [1.00000000]\n",
      "    fold  1:  [0.90000000]\n",
      "    fold  2:  [1.00000000]\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.95000000] + [0.05000000]\n",
      "    FULL:     [0.95000000]\n",
      "\n",
      "model  1:     [RandomForestClassifier]\n",
      "    fold  0:  [0.96666667]\n",
      "    fold  1:  [0.90000000]\n",
      "    fold  2:  [1.00000000]\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.94166667] + [0.04330127]\n",
      "    FULL:     [0.94166667]\n",
      "\n",
      "model  2:     [XGBClassifier]\n",
      "[13:35:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  0:  [0.93333333]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:35:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  1:  [0.90000000]\n",
      "[13:35:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  2:  [1.00000000]\n",
      "[13:35:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.93333333] + [0.04082483]\n",
      "    FULL:     [0.93333333]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Compute stacking features\n",
    "\n",
    "# Training data x & y 에 대해 3가지 모델들을 모두 학습시킨 후,\n",
    "# 1) 해당 모델들이 X_train 에 대해 예측한 결과를 Data point 각각에 대해 묶어서 돌려준다. (S_train)\n",
    "# : Shape of X_train == (120, 4) -> Shape of S_train == (120, 3) <- 모델 각각의 예측치가 하나의 열\n",
    "# 2) 해당 모델들이 X_test 에 대해 예측한 결과를 Data point 각각에 대해 묶어서 돌려준다. (S_test)\n",
    "# : Shape of X_test == (30, 4) -> Shape of S_test == (30, 3) <- 모델 각각의 예측치가 하나의 열\n",
    "\n",
    "S_train, S_test = stacking(models, \n",
    "                           X_train, y_train, X_test, \n",
    "                           regression = False, \n",
    "                           metric = accuracy_score, \n",
    "                           n_folds = 4, stratified = True, shuffle = True, \n",
    "                           random_state = 0, verbose = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 2-nd level model \n",
    "\n",
    "model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric='mlogloss') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit 2-nd level model \n",
    "# 3개의 모델이 예측한 결과인 S_train을 Feature(3개의 열)로 하여 y_train을 맞추도록 모델을 학습시킨다.\n",
    "\n",
    "model = model.fit(S_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict \n",
    "# 앞서 3개의 모델이 예측한 결과인 S_test를 Feature로 하여 y_test를 예측한다.\n",
    "\n",
    "y_pred = model.predict(S_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction score: [0.96666667]\n"
     ]
    }
   ],
   "source": [
    "# Final prediction score \n",
    "\n",
    "print('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 2. Usage. Scikit-learn API (권장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vecstack import StackingTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your data\n",
    "\n",
    "iris = load_iris() \n",
    "X, y = iris.data, iris.target \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 1st level estimators\n",
    "\n",
    "estimators = [ \n",
    "    ('ExtraTrees', ExtraTreesClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3)),\n",
    "    ('RandomForest', RandomForestClassifier(random_state = 0, n_jobs = -1, n_estimators = 100, max_depth = 3)),\n",
    "    ('XGB', XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric='mlogloss'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize StackingTransformer\n",
    "\n",
    "stack = StackingTransformer(estimators, \n",
    "                            regression = False, \n",
    "                            metric = accuracy_score, \n",
    "                            n_folds = 4, stratified = True, shuffle = True, \n",
    "                            random_state = 0, verbose = 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [accuracy_score]\n",
      "variant:      [A]\n",
      "n_estimators: [3]\n",
      "\n",
      "estimator  0: [ExtraTrees: ExtraTreesClassifier]\n",
      "    fold  0:  [1.00000000]\n",
      "    fold  1:  [0.90000000]\n",
      "    fold  2:  [1.00000000]\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.95000000] + [0.05000000]\n",
      "\n",
      "estimator  1: [RandomForest: RandomForestClassifier]\n",
      "    fold  0:  [0.96666667]\n",
      "    fold  1:  [0.90000000]\n",
      "    fold  2:  [1.00000000]\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.94166667] + [0.04330127]\n",
      "\n",
      "estimator  2: [XGB: XGBClassifier]\n",
      "    fold  0:  [0.93333333]\n",
      "    fold  1:  [0.90000000]\n",
      "    fold  2:  [1.00000000]\n",
      "    fold  3:  [0.90000000]\n",
      "    ----\n",
      "    MEAN:     [0.93333333] + [0.04082483]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit\n",
    "\n",
    "stack = stack.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set was detected.\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [ExtraTrees: ExtraTreesClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [RandomForest: RandomForestClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [XGB: XGBClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "Transforming...\n",
      "\n",
      "estimator  0: [ExtraTrees: ExtraTreesClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  1: [RandomForest: RandomForestClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n",
      "estimator  2: [XGB: XGBClassifier]\n",
      "    model from fold  0: done\n",
      "    model from fold  1: done\n",
      "    model from fold  2: done\n",
      "    model from fold  3: done\n",
      "    ----\n",
      "    DONE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get your stacked features\n",
    "\n",
    "S_train = stack.transform(X_train)\n",
    "S_test = stack.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction score: [0.96666667]\n"
     ]
    }
   ],
   "source": [
    "# Use 2nd level estimator with stacked features\n",
    "\n",
    "model = XGBClassifier(seed = 0, n_jobs = -1, learning_rate = 0.1, n_estimators = 100, max_depth = 3, eval_metric='mlogloss') \n",
    "model = model.fit(S_train, y_train) \n",
    "\n",
    "y_pred = model.predict(S_test) \n",
    "print('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
